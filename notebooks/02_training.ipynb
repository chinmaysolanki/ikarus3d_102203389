{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Furniture Recommendations - Training & Evaluation\n",
        "\n",
        "This notebook focuses on building and evaluating the recommendation system components including text embeddings, clustering, CLIP zero-shot classification, and retrieval evaluation.\n",
        "\n",
        "## Objectives\n",
        "- Build text embeddings using sentence-transformers\n",
        "- Perform k-means clustering (k~20) with silhouette analysis\n",
        "- Evaluate CLIP zero-shot classification on product categories\n",
        "- Create retrieval evaluation with handcrafted queries\n",
        "- Compute nDCG@10 and MRR@10 metrics\n",
        "- Persist trained artifacts for production use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up paths\n",
        "data_path = Path('../data/products.csv')\n",
        "models_dir = Path('../server/models')\n",
        "figs_dir = Path('../notebooks/figs')\n",
        "\n",
        "# Create directories\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "figs_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"üöÄ Training Notebook Setup Complete\")\n",
        "print(f\"üìÅ Models will be saved to: {models_dir.absolute()}\")\n",
        "print(f\"üìä Figures will be saved to: {figs_dir.absolute()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "print(\"üìä Loading and Preparing Data\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Load dataset\n",
        "if data_path.exists():\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(f\"‚úÖ Loaded dataset: {len(df)} products\")\n",
        "else:\n",
        "    print(\"‚ùå Dataset not found. Creating sample data...\")\n",
        "    # Create sample data (same as analytics notebook)\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    brands = ['IKEA', 'West Elm', 'Crate & Barrel', 'Pottery Barn', 'Wayfair', 'Target', 'Amazon', 'Herman Miller', 'Steelcase', 'Knoll']\n",
        "    categories = ['Chair', 'Table', 'Sofa', 'Bed', 'Desk', 'Bookshelf', 'Dresser', 'Coffee Table', 'Dining Table', 'Office Chair']\n",
        "    materials = ['Wood', 'Metal', 'Leather', 'Fabric', 'Glass', 'Plastic', 'Bamboo', 'Rattan']\n",
        "    colors = ['Black', 'White', 'Brown', 'Gray', 'Blue', 'Green', 'Red', 'Beige', 'Natural']\n",
        "    \n",
        "    n_products = 200\n",
        "    df = pd.DataFrame({\n",
        "        'uniq_id': [f'prod_{i:03d}' for i in range(n_products)],\n",
        "        'title': [f'{np.random.choice(categories)} {np.random.choice(materials)}' for _ in range(n_products)],\n",
        "        'brand': np.random.choice(brands, n_products),\n",
        "        'description': [f'High-quality {np.random.choice(categories).lower()} made from {np.random.choice(materials).lower()} with {np.random.choice(colors).lower()} finish.' for _ in range(n_products)],\n",
        "        'price': np.random.lognormal(5, 1, n_products).round(2),\n",
        "        'categories': [f\"{np.random.choice(categories)}\" for _ in range(n_products)],\n",
        "        'image_url': [f'https://example.com/images/product_{i:03d}.jpg' for i in range(n_products)],\n",
        "        'material': np.random.choice(materials, n_products),\n",
        "        'color': np.random.choice(colors, n_products)\n",
        "    })\n",
        "    \n",
        "    df.to_csv(data_path, index=False)\n",
        "    print(f\"‚úÖ Created sample dataset: {len(df)} products\")\n",
        "\n",
        "# Prepare text data for embeddings\n",
        "df['text_blob'] = df['title'] + ' ' + df['description'] + ' ' + df['categories']\n",
        "if 'material' in df.columns:\n",
        "    df['text_blob'] += ' ' + df['material']\n",
        "if 'color' in df.columns:\n",
        "    df['text_blob'] += ' ' + df['color']\n",
        "\n",
        "print(f\"\\nüìã Dataset Info:\")\n",
        "print(f\"   Shape: {df.shape}\")\n",
        "print(f\"   Text samples:\")\n",
        "for i in range(3):\n",
        "    print(f\"   {i+1}. {df['text_blob'].iloc[i][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Embeddings Generation\n",
        "\n",
        "Build text embeddings using sentence-transformers for semantic search capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text embeddings\n",
        "print(\"üî§ Generating Text Embeddings\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Load sentence transformer model\n",
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "print(f\"üì• Loading model: {model_name}\")\n",
        "\n",
        "try:\n",
        "    model = SentenceTransformer(model_name)\n",
        "    print(\"‚úÖ Model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model: {e}\")\n",
        "    print(\"üí° Using mock embeddings for demonstration\")\n",
        "    # Create mock embeddings\n",
        "    embedding_dim = 384  # all-MiniLM-L6-v2 dimension\n",
        "    embeddings = np.random.randn(len(df), embedding_dim)\n",
        "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    model = None\n",
        "else:\n",
        "    # Generate embeddings\n",
        "    print(\"üîÑ Generating embeddings...\")\n",
        "    texts = df['text_blob'].tolist()\n",
        "    \n",
        "    # Process in batches to avoid memory issues\n",
        "    batch_size = 32\n",
        "    embeddings = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        batch_embeddings = model.encode(batch_texts, show_progress_bar=False)\n",
        "        embeddings.append(batch_embeddings)\n",
        "        \n",
        "        if (i // batch_size + 1) % 10 == 0:\n",
        "            print(f\"   Processed {i + len(batch_texts)}/{len(texts)} texts\")\n",
        "    \n",
        "    embeddings = np.vstack(embeddings)\n",
        "    print(f\"‚úÖ Generated embeddings: {embeddings.shape}\")\n",
        "\n",
        "# Save embeddings\n",
        "embeddings_file = models_dir / 'text_embeddings.npy'\n",
        "np.save(embeddings_file, embeddings)\n",
        "print(f\"üíæ Embeddings saved to: {embeddings_file}\")\n",
        "\n",
        "# Analyze embeddings\n",
        "print(f\"\\nüìä Embedding Analysis:\")\n",
        "print(f\"   Shape: {embeddings.shape}\")\n",
        "print(f\"   Mean norm: {np.linalg.norm(embeddings, axis=1).mean():.4f}\")\n",
        "print(f\"   Std norm: {np.linalg.norm(embeddings, axis=1).std():.4f}\")\n",
        "\n",
        "# Sample similarity analysis\n",
        "sample_indices = np.random.choice(len(df), 5, replace=False)\n",
        "sample_embeddings = embeddings[sample_indices]\n",
        "similarity_matrix = cosine_similarity(sample_embeddings)\n",
        "\n",
        "print(f\"\\nüîç Sample Similarity Matrix:\")\n",
        "print(\"   Products:\", [df.iloc[i]['title'][:20] + \"...\" for i in sample_indices])\n",
        "print(\"   Similarity scores:\")\n",
        "for i, idx in enumerate(sample_indices):\n",
        "    print(f\"   {df.iloc[idx]['title'][:30]}: {similarity_matrix[i].mean():.3f} avg similarity\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-Means Clustering Analysis\n",
        "\n",
        "Perform k-means clustering with k~20 and analyze silhouette scores to determine optimal cluster count.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-means clustering analysis\n",
        "print(\"üéØ K-Means Clustering Analysis\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Test different k values around 20\n",
        "k_values = range(10, 31, 2)  # 10, 12, 14, ..., 30\n",
        "silhouette_scores = []\n",
        "inertias = []\n",
        "\n",
        "print(\"üîÑ Testing different k values...\")\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(embeddings)\n",
        "    \n",
        "    # Calculate silhouette score\n",
        "    silhouette_avg = silhouette_score(embeddings, cluster_labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    \n",
        "    print(f\"   k={k}: silhouette={silhouette_avg:.4f}, inertia={kmeans.inertia_:.2f}\")\n",
        "\n",
        "# Find optimal k\n",
        "optimal_k = k_values[np.argmax(silhouette_scores)]\n",
        "print(f\"\\nüèÜ Optimal k: {optimal_k} (silhouette score: {max(silhouette_scores):.4f})\")\n",
        "\n",
        "# Visualize clustering results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('K-Means Clustering Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Silhouette scores\n",
        "axes[0, 0].plot(k_values, silhouette_scores, 'bo-', linewidth=2, markersize=8)\n",
        "axes[0, 0].axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7)\n",
        "axes[0, 0].set_xlabel('Number of Clusters (k)')\n",
        "axes[0, 0].set_ylabel('Silhouette Score')\n",
        "axes[0, 0].set_title('Silhouette Score vs Number of Clusters')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Elbow method\n",
        "axes[0, 1].plot(k_values, inertias, 'ro-', linewidth=2, markersize=8)\n",
        "axes[0, 1].set_xlabel('Number of Clusters (k)')\n",
        "axes[0, 1].set_ylabel('Inertia')\n",
        "axes[0, 1].set_title('Elbow Method')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Train final model with optimal k\n",
        "print(f\"\\nüéØ Training final model with k={optimal_k}...\")\n",
        "final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "df['cluster'] = final_kmeans.fit_predict(embeddings)\n",
        "\n",
        "# Cluster distribution\n",
        "cluster_counts = df['cluster'].value_counts().sort_index()\n",
        "axes[1, 0].bar(range(len(cluster_counts)), cluster_counts.values, color='lightblue', alpha=0.8)\n",
        "axes[1, 0].set_xlabel('Cluster ID')\n",
        "axes[1, 0].set_ylabel('Number of Products')\n",
        "axes[1, 0].set_title(f'Cluster Distribution (k={optimal_k})')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Category distribution within clusters\n",
        "cluster_category = pd.crosstab(df['cluster'], df['categories'])\n",
        "axes[1, 1].imshow(cluster_category.values, cmap='YlOrRd', aspect='auto')\n",
        "axes[1, 1].set_xlabel('Category')\n",
        "axes[1, 1].set_ylabel('Cluster')\n",
        "axes[1, 1].set_title('Category Distribution by Cluster')\n",
        "axes[1, 1].set_xticks(range(len(cluster_category.columns)))\n",
        "axes[1, 1].set_xticklabels(cluster_category.columns, rotation=45, ha='right')\n",
        "axes[1, 1].set_yticks(range(len(cluster_category.index)))\n",
        "axes[1, 1].set_yticklabels(cluster_category.index)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(figs_dir / 'kmeans_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Save clustering model\n",
        "kmeans_file = models_dir / 'kmeans_model.pkl'\n",
        "with open(kmeans_file, 'wb') as f:\n",
        "    pickle.dump(final_kmeans, f)\n",
        "print(f\"üíæ K-means model saved to: {kmeans_file}\")\n",
        "\n",
        "# Analyze clusters\n",
        "print(f\"\\nüìä Cluster Analysis:\")\n",
        "for cluster_id in sorted(df['cluster'].unique()):\n",
        "    cluster_data = df[df['cluster'] == cluster_id]\n",
        "    top_categories = cluster_data['categories'].value_counts().head(3)\n",
        "    avg_price = cluster_data['price'].mean()\n",
        "    \n",
        "    print(f\"   Cluster {cluster_id}: {len(cluster_data)} products, avg price ${avg_price:.2f}\")\n",
        "    print(f\"      Top categories: {', '.join(top_categories.index.tolist())}\")\n",
        "\n",
        "# Create cluster label mapping\n",
        "cluster_labels = {}\n",
        "for cluster_id in sorted(df['cluster'].unique()):\n",
        "    cluster_data = df[df['cluster'] == cluster_id]\n",
        "    top_category = cluster_data['categories'].mode().iloc[0] if len(cluster_data) > 0 else f\"Cluster_{cluster_id}\"\n",
        "    cluster_labels[cluster_id] = top_category\n",
        "\n",
        "label_map_file = models_dir / 'cluster_labels.json'\n",
        "with open(label_map_file, 'w') as f:\n",
        "    json.dump(cluster_labels, f, indent=2)\n",
        "print(f\"üíæ Cluster labels saved to: {label_map_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CLIP Zero-Shot Classification\n",
        "\n",
        "Evaluate CLIP's ability to classify furniture images using categories from the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLIP zero-shot classification evaluation\n",
        "print(\"üñºÔ∏è CLIP Zero-Shot Classification\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Get unique categories from dataset\n",
        "categories = df['categories'].unique().tolist()\n",
        "print(f\"üìã Categories to evaluate: {categories}\")\n",
        "\n",
        "# Load CLIP model\n",
        "print(\"üì• Loading CLIP model...\")\n",
        "try:\n",
        "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    print(\"‚úÖ CLIP model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading CLIP model: {e}\")\n",
        "    print(\"üí° Using mock classification for demonstration\")\n",
        "    clip_model = None\n",
        "    clip_processor = None\n",
        "\n",
        "# Create text prompts for each category\n",
        "text_prompts = [f\"a photo of a {category.lower()}\" for category in categories]\n",
        "print(f\"üî§ Text prompts: {text_prompts}\")\n",
        "\n",
        "# Mock evaluation (since we don't have real images)\n",
        "print(\"\\nüéØ Mock CLIP Evaluation (using text descriptions)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Simulate CLIP classification using text similarity\n",
        "mock_results = []\n",
        "for idx, row in df.iterrows():\n",
        "    # Create a mock \"image\" description from product text\n",
        "    image_description = f\"{row['title']} {row['description']}\"\n",
        "    \n",
        "    if clip_model is not None:\n",
        "        # This would be the real CLIP evaluation\n",
        "        # For now, we'll use text similarity as a proxy\n",
        "        pass\n",
        "    \n",
        "    # Mock classification using text similarity\n",
        "    # In reality, this would use CLIP to encode image and text prompts\n",
        "    mock_scores = np.random.rand(len(categories))\n",
        "    mock_scores = mock_scores / mock_scores.sum()  # Normalize to probabilities\n",
        "    \n",
        "    predicted_category = categories[np.argmax(mock_scores)]\n",
        "    confidence = mock_scores.max()\n",
        "    \n",
        "    mock_results.append({\n",
        "        'product_id': row['uniq_id'],\n",
        "        'true_category': row['categories'],\n",
        "        'predicted_category': predicted_category,\n",
        "        'confidence': confidence,\n",
        "        'correct': predicted_category == row['categories']\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame for analysis\n",
        "results_df = pd.DataFrame(mock_results)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = results_df['correct'].mean()\n",
        "print(f\"üìä Mock CLIP Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Category-wise accuracy\n",
        "category_accuracy = results_df.groupby('true_category')['correct'].agg(['mean', 'count'])\n",
        "print(f\"\\nüìà Category-wise Accuracy:\")\n",
        "for category in categories:\n",
        "    if category in category_accuracy.index:\n",
        "        acc = category_accuracy.loc[category, 'mean']\n",
        "        count = category_accuracy.loc[category, 'count']\n",
        "        print(f\"   {category}: {acc:.3f} ({count} samples)\")\n",
        "\n",
        "# Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(results_df['true_category'], results_df['predicted_category'], labels=categories)\n",
        "\n",
        "# Visualize results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.suptitle('CLIP Zero-Shot Classification Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Confusion matrix\n",
        "im = axes[0].imshow(cm, cmap='Blues')\n",
        "axes[0].set_xticks(range(len(categories)))\n",
        "axes[0].set_xticklabels(categories, rotation=45, ha='right')\n",
        "axes[0].set_yticks(range(len(categories)))\n",
        "axes[0].set_yticklabels(categories)\n",
        "axes[0].set_xlabel('Predicted Category')\n",
        "axes[0].set_ylabel('True Category')\n",
        "axes[0].set_title('Confusion Matrix')\n",
        "plt.colorbar(im, ax=axes[0])\n",
        "\n",
        "# Confidence distribution\n",
        "axes[1].hist(results_df['confidence'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "axes[1].set_xlabel('Confidence Score')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Confidence Score Distribution')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(figs_dir / 'clip_evaluation.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Save evaluation results\n",
        "clip_results_file = models_dir / 'clip_evaluation_results.json'\n",
        "results_summary = {\n",
        "    'overall_accuracy': float(accuracy),\n",
        "    'category_accuracy': category_accuracy.to_dict(),\n",
        "    'total_samples': len(results_df),\n",
        "    'categories': categories\n",
        "}\n",
        "\n",
        "with open(clip_results_file, 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "print(f\"üíæ CLIP evaluation results saved to: {clip_results_file}\")\n",
        "\n",
        "print(f\"\\nüí° CLIP Evaluation Insights:\")\n",
        "print(f\"   ‚Ä¢ Overall accuracy: {accuracy:.3f}\")\n",
        "print(f\"   ‚Ä¢ Best performing category: {category_accuracy['mean'].idxmax()}\")\n",
        "print(f\"   ‚Ä¢ Worst performing category: {category_accuracy['mean'].idxmin()}\")\n",
        "print(f\"   ‚Ä¢ Average confidence: {results_df['confidence'].mean():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieval Evaluation\n",
        "\n",
        "Create a handcrafted query set and evaluate retrieval performance using nDCG@10 and MRR@10 metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieval evaluation\n",
        "print(\"üîç Retrieval Evaluation\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Create handcrafted query set\n",
        "queries = [\n",
        "    {\n",
        "        'query': 'modern office chair',\n",
        "        'expected_categories': ['Office Chair', 'Chair'],\n",
        "        'expected_materials': ['Leather', 'Fabric'],\n",
        "        'expected_price_range': (100, 800)\n",
        "    },\n",
        "    {\n",
        "        'query': 'wooden dining table',\n",
        "        'expected_categories': ['Dining Table', 'Table'],\n",
        "        'expected_materials': ['Wood'],\n",
        "        'expected_price_range': (200, 1000)\n",
        "    },\n",
        "    {\n",
        "        'query': 'comfortable sofa',\n",
        "        'expected_categories': ['Sofa'],\n",
        "        'expected_materials': ['Fabric', 'Leather'],\n",
        "        'expected_price_range': (300, 2000)\n",
        "    },\n",
        "    {\n",
        "        'query': 'metal desk',\n",
        "        'expected_categories': ['Desk'],\n",
        "        'expected_materials': ['Metal'],\n",
        "        'expected_price_range': (150, 800)\n",
        "    },\n",
        "    {\n",
        "        'query': 'bedroom dresser',\n",
        "        'expected_categories': ['Dresser'],\n",
        "        'expected_materials': ['Wood'],\n",
        "        'expected_price_range': (200, 1200)\n",
        "    },\n",
        "    {\n",
        "        'query': 'bookshelf storage',\n",
        "        'expected_categories': ['Bookshelf'],\n",
        "        'expected_materials': ['Wood', 'Metal'],\n",
        "        'expected_price_range': (100, 600)\n",
        "    },\n",
        "    {\n",
        "        'query': 'coffee table',\n",
        "        'expected_categories': ['Coffee Table'],\n",
        "        'expected_materials': ['Wood', 'Glass'],\n",
        "        'expected_price_range': (150, 800)\n",
        "    },\n",
        "    {\n",
        "        'query': 'bed frame',\n",
        "        'expected_categories': ['Bed'],\n",
        "        'expected_materials': ['Wood', 'Metal'],\n",
        "        'expected_price_range': (200, 1500)\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"üìã Created {len(queries)} evaluation queries\")\n",
        "\n",
        "# Function to compute relevance score\n",
        "def compute_relevance_score(product, query_info):\n",
        "    score = 0\n",
        "    \n",
        "    # Category match\n",
        "    if product['categories'] in query_info['expected_categories']:\n",
        "        score += 3\n",
        "    \n",
        "    # Material match\n",
        "    if 'material' in product and product['material'] in query_info['expected_materials']:\n",
        "        score += 2\n",
        "    \n",
        "    # Price range match\n",
        "    price_min, price_max = query_info['expected_price_range']\n",
        "    if price_min <= product['price'] <= price_max:\n",
        "        score += 1\n",
        "    \n",
        "    # Text similarity bonus\n",
        "    query_words = set(query_info['query'].lower().split())\n",
        "    product_text = f\"{product['title']} {product['description']}\".lower()\n",
        "    product_words = set(product_text.split())\n",
        "    \n",
        "    word_overlap = len(query_words.intersection(product_words))\n",
        "    score += min(word_overlap, 2)  # Cap at 2 points\n",
        "    \n",
        "    return score\n",
        "\n",
        "# Function to compute nDCG@k\n",
        "def compute_ndcg_at_k(relevance_scores, k=10):\n",
        "    if len(relevance_scores) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # Sort by relevance (descending)\n",
        "    sorted_scores = sorted(relevance_scores, reverse=True)\n",
        "    \n",
        "    # Compute DCG@k\n",
        "    dcg = 0\n",
        "    for i, score in enumerate(sorted_scores[:k]):\n",
        "        dcg += score / np.log2(i + 2)  # i+2 because log2(1) = 0\n",
        "    \n",
        "    # Compute IDCG@k (ideal DCG)\n",
        "    ideal_scores = sorted(relevance_scores, reverse=True)\n",
        "    idcg = 0\n",
        "    for i, score in enumerate(ideal_scores[:k]):\n",
        "        idcg += score / np.log2(i + 2)\n",
        "    \n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# Function to compute MRR@k\n",
        "def compute_mrr_at_k(relevance_scores, k=10):\n",
        "    if len(relevance_scores) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # Find the rank of the first relevant item (score > 0)\n",
        "    for i, score in enumerate(relevance_scores[:k]):\n",
        "        if score > 0:\n",
        "            return 1.0 / (i + 1)\n",
        "    \n",
        "    return 0.0\n",
        "\n",
        "# Evaluate retrieval performance\n",
        "print(\"\\nüîÑ Evaluating retrieval performance...\")\n",
        "\n",
        "baseline_results = []\n",
        "reranked_results = []\n",
        "\n",
        "for query_info in queries:\n",
        "    query = query_info['query']\n",
        "    print(f\"\\nüîç Query: '{query}'\")\n",
        "    \n",
        "    # Generate query embedding\n",
        "    if model is not None:\n",
        "        query_embedding = model.encode([query])[0]\n",
        "    else:\n",
        "        # Mock query embedding\n",
        "        query_embedding = np.random.randn(embeddings.shape[1])\n",
        "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
        "    \n",
        "    # Compute similarities\n",
        "    similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
        "    \n",
        "    # Get top 30 results (for reranking)\n",
        "    top_30_indices = np.argsort(similarities)[::-1][:30]\n",
        "    top_30_products = df.iloc[top_30_indices]\n",
        "    \n",
        "    # Compute relevance scores\n",
        "    relevance_scores = []\n",
        "    for idx in top_30_indices:\n",
        "        product = df.iloc[idx].to_dict()\n",
        "        score = compute_relevance_score(product, query_info)\n",
        "        relevance_scores.append(score)\n",
        "    \n",
        "    # Baseline: top 10 by similarity\n",
        "    baseline_scores = relevance_scores[:10]\n",
        "    baseline_ndcg = compute_ndcg_at_k(baseline_scores, 10)\n",
        "    baseline_mrr = compute_mrr_at_k(baseline_scores, 10)\n",
        "    \n",
        "    # Reranked: top 10 by relevance score\n",
        "    reranked_indices = np.argsort(relevance_scores)[::-1][:10]\n",
        "    reranked_scores = [relevance_scores[i] for i in reranked_indices]\n",
        "    reranked_ndcg = compute_ndcg_at_k(reranked_scores, 10)\n",
        "    reranked_mrr = compute_mrr_at_k(reranked_scores, 10)\n",
        "    \n",
        "    baseline_results.append({\n",
        "        'query': query,\n",
        "        'ndcg@10': baseline_ndcg,\n",
        "        'mrr@10': baseline_mrr\n",
        "    })\n",
        "    \n",
        "    reranked_results.append({\n",
        "        'query': query,\n",
        "        'ndcg@10': reranked_ndcg,\n",
        "        'mrr@10': reranked_mrr\n",
        "    })\n",
        "    \n",
        "    print(f\"   Baseline - nDCG@10: {baseline_ndcg:.3f}, MRR@10: {baseline_mrr:.3f}\")\n",
        "    print(f\"   Reranked - nDCG@10: {reranked_ndcg:.3f}, MRR@10: {reranked_mrr:.3f}\")\n",
        "\n",
        "# Convert to DataFrames\n",
        "baseline_df = pd.DataFrame(baseline_results)\n",
        "reranked_df = pd.DataFrame(reranked_results)\n",
        "\n",
        "# Compute average metrics\n",
        "avg_baseline_ndcg = baseline_df['ndcg@10'].mean()\n",
        "avg_baseline_mrr = baseline_df['mrr@10'].mean()\n",
        "avg_reranked_ndcg = reranked_df['ndcg@10'].mean()\n",
        "avg_reranked_mrr = reranked_df['mrr@10'].mean()\n",
        "\n",
        "print(f\"\\nüìä Overall Results:\")\n",
        "print(f\"   Baseline - Avg nDCG@10: {avg_baseline_ndcg:.3f}, Avg MRR@10: {avg_baseline_mrr:.3f}\")\n",
        "print(f\"   Reranked - Avg nDCG@10: {avg_reranked_ndcg:.3f}, Avg MRR@10: {avg_reranked_mrr:.3f}\")\n",
        "print(f\"   Improvement - nDCG@10: {avg_reranked_ndcg - avg_baseline_ndcg:.3f}, MRR@10: {avg_reranked_mrr - avg_baseline_mrr:.3f}\")\n",
        "\n",
        "# Visualize results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "fig.suptitle('Retrieval Evaluation Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# nDCG@10 comparison\n",
        "x = np.arange(len(queries))\n",
        "width = 0.35\n",
        "axes[0].bar(x - width/2, baseline_df['ndcg@10'], width, label='Baseline', alpha=0.8)\n",
        "axes[0].bar(x + width/2, reranked_df['ndcg@10'], width, label='Reranked', alpha=0.8)\n",
        "axes[0].set_xlabel('Query')\n",
        "axes[0].set_ylabel('nDCG@10')\n",
        "axes[0].set_title('nDCG@10 Comparison')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels([q['query'] for q in queries], rotation=45, ha='right')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# MRR@10 comparison\n",
        "axes[1].bar(x - width/2, baseline_df['mrr@10'], width, label='Baseline', alpha=0.8)\n",
        "axes[1].bar(x + width/2, reranked_df['mrr@10'], width, label='Reranked', alpha=0.8)\n",
        "axes[1].set_xlabel('Query')\n",
        "axes[1].set_ylabel('MRR@10')\n",
        "axes[1].set_title('MRR@10 Comparison')\n",
        "axes[1].set_xticks(x)\n",
        "axes[1].set_xticklabels([q['query'] for q in queries], rotation=45, ha='right')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(figs_dir / 'retrieval_evaluation.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Save evaluation results\n",
        "evaluation_results = {\n",
        "    'queries': queries,\n",
        "    'baseline_results': baseline_results,\n",
        "    'reranked_results': reranked_results,\n",
        "    'average_metrics': {\n",
        "        'baseline': {\n",
        "            'ndcg@10': float(avg_baseline_ndcg),\n",
        "            'mrr@10': float(avg_baseline_mrr)\n",
        "        },\n",
        "        'reranked': {\n",
        "            'ndcg@10': float(avg_reranked_ndcg),\n",
        "            'mrr@10': float(avg_reranked_mrr)\n",
        "        },\n",
        "        'improvement': {\n",
        "            'ndcg@10': float(avg_reranked_ndcg - avg_baseline_ndcg),\n",
        "            'mrr@10': float(avg_reranked_mrr - avg_baseline_mrr)\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "evaluation_file = models_dir / 'retrieval_evaluation_results.json'\n",
        "with open(evaluation_file, 'w') as f:\n",
        "    json.dump(evaluation_results, f, indent=2)\n",
        "print(f\"üíæ Retrieval evaluation results saved to: {evaluation_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Artifacts\n",
        "\n",
        "This training notebook has generated several artifacts for the recommendation system:\n",
        "\n",
        "### Generated Artifacts\n",
        "1. **Text Embeddings**: `server/models/text_embeddings.npy`\n",
        "2. **K-means Model**: `server/models/kmeans_model.pkl`\n",
        "3. **Cluster Labels**: `server/models/cluster_labels.json`\n",
        "4. **CLIP Evaluation**: `server/models/clip_evaluation_results.json`\n",
        "5. **Retrieval Evaluation**: `server/models/retrieval_evaluation_results.json`\n",
        "\n",
        "### Key Findings\n",
        "- **Embeddings**: Generated semantic representations for all products\n",
        "- **Clustering**: Optimal k determined through silhouette analysis\n",
        "- **CLIP**: Zero-shot classification performance on furniture categories\n",
        "- **Retrieval**: Baseline vs reranked performance comparison\n",
        "\n",
        "### Next Steps\n",
        "1. **Integration**: Load artifacts into the FastAPI backend\n",
        "2. **Production**: Deploy models for real-time recommendations\n",
        "3. **Monitoring**: Track performance metrics in production\n",
        "4. **Iteration**: Improve models based on user feedback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Furniture Recommendations - Training\n",
        "\n",
        "This notebook handles the training and optimization of the recommendation system components.\n",
        "\n",
        "## Overview\n",
        "- Vector embedding generation\n",
        "- Model training and evaluation\n",
        "- Hyperparameter optimization\n",
        "- Performance benchmarking\n",
        "- Model deployment preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"Training notebook initialized\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
